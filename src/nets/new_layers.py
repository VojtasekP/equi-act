
from escnn.group import *
from escnn.gspaces import *
from escnn.nn import FieldType
from escnn.nn import GeometricTensor
from escnn.nn import R2Conv
from escnn.nn.modules.equivariant_module import EquivariantModule
from escnn.nn import SequentialModule
import escnn.nn as nn  # to avoid shadowing torch.nn as nn
import escnn.gspaces as gspaces
import torch
import torch.nn.functional as F

from typing import List, Tuple, Any

import numpy as np



def _build_kernel(G: Group, irrep: List[tuple]):
    kernel = []
    
    for irr in irrep:
        irr = G.irrep(*irr)
        
        c = int(irr.size//irr.sum_of_squares_constituents)
        k = irr(G.identity)[:, :c] * np.sqrt(irr.size)
        kernel.append(k.T.reshape(-1))
    
    kernel = np.concatenate(kernel)
    return kernel
    

class AdaptiveFourierPointwiseSO2(nn.Module):
    """
    Adaptive Fourier-based nonlinearity for SO(2) with per-location sampling matrices A(x).

    Pipeline at each spatial location x:
        s(x) = A(x) @ f̂(x)          # inverse FT samples at N angles
        y(x) = σ(s(x))               # pointwise nonlinearity
        f̂'(x) = (1/N) A(x)^T @ y(x) # fast pseudo-inverse (rows ~ orthonormal)

    A(x) is generated by an equivariant sampling branch (one R2Conv -> type-1), then
    N angles are built as θ_i(x) = θ0(x) + Δ_i, with fixed offsets Δ_i to preserve equivariance.
    """
    def __init__(
        self,
        space: gspaces.Rot2dOnR2,
        channels: int,
        irreps,                          # the same object you pass to your FourierPointwise
        *,
        function: str = 'p_elu',
        inplace: bool = True,
        normalize: bool = True,
        N: int = 8,
        offsets: torch.Tensor | None = None,
        input_type_irreps: FieldType | None = None,
    ):
        super().__init__()
        self.space = space
        G = self.space.fibergroup

        # Build the representation used in the nonlinearity (regular/quotient in SO(2) case).
        # Must match your Fourier layer construction.
        self.rho = G.spectral_regular_representation(*irreps)
        self.rho_out = self.rho  # same bandlimit; extend if you need out_irreps

        # ESCNN type metadata
        self.in_type = FieldType(self.space, [self.rho] * channels)
        self.out_type = FieldType(self.space, [self.rho_out] * channels)

        # Pointwise function (use non-deprecated funcs, avoid in-place for safety with autograd)
        if function == 'p_relu':
            self._function = F.relu if not inplace else torch.relu_
        elif function == 'p_elu':
            self._function = F.elu if not inplace else torch.elu_
        elif function == 'p_sigmoid':
            self._function = torch.sigmoid  # no in-place variant
        elif function == 'p_tanh':
            self._function = torch.tanh     # no in-place variant
        else:
            raise ValueError(f'Function "{function}" not recognized!')

        # Dirac kernel in rep space: δ̂, same object your fixed-grid FourierPointwise uses
        self.kernel = _build_kernel(G, irreps)  # (F, 1) after reshape below
        assert self.kernel.shape[0] == self.rho.size, "kernel and representation size mismatch"
        if normalize:
            self.kernel = self.kernel / np.linalg.norm(self.kernel)
        self.register_buffer('kernel_t', torch.tensor(self.kernel.reshape(-1, 1), dtype=torch.get_default_dtype()))

        # Sampling spec
        self.N = int(N)
        if offsets is None:
            offs = torch.linspace(0, 2 * torch.pi, self.N + 1)[:-1]
        else:
            offs = offsets.detach()
            if offs.shape[0] != self.N:
                raise ValueError("offsets length must equal N")
        self.register_buffer('offsets', offs)  # shape (N,)

        # One-conv equivariant sampling branch -> type-1 output (2 channels) to extract θ via atan2
        # Your draft wrongly used out_type=input_type_irreps which is not a type-1 vector. 
        if input_type_irreps is None:
            input_type_irreps = self.in_type
        vec_type = FieldType(self.space, [self.space.fibergroup.irrep(1)])  # 2D real rep rotates like a vector
        self.sampling_branch = SequentialModule(
            R2Conv(input_type=input_type_irreps, out_type=vec_type, kernel_size=1, bias=False)
        )

    # ---------- grid construction ----------

    @torch.no_grad()
    def _angles_from_vector(self, v: torch.Tensor) -> torch.Tensor:
        """
        v: (B, 2, H, W) type-1 field
        returns thetas: (B, H, W) base angles in [−π, π]
        """
        # small floor to avoid NaN atan2 at zero
        vx, vy = v[:, 0], v[:, 1]
        return torch.atan2(vy, vx)

    def _build_A_rows_batch(self, thetas: torch.Tensor) -> torch.Tensor:
        """
        Build per-location sampling matrix rows using rho(g) @ δ̂, which
        automatically respects the internal column ordering of self.rho.

        thetas: (B, H, W, N)
        returns A: (B, H, W, N, F)
        """
        B, H, W, N = thetas.shape
        F = self.rho.size
        device = thetas.device
        A = torch.empty((B, H, W, N, F), dtype=self.kernel_t.dtype, device=device)

        # This uses python-level loops over N (and B,H,W implicit through indexing)
        # because escnn’s rho(g) returns numpy arrays per element. Correct > fast.
        for n in range(N):
            theta_n = thetas[..., n]  # (B, H, W)
            # We have different θ per pixel. Build A row per pixel by calling rho(gθ) @ δ̂.
            # Unavoidably a loop over all pixels if we call rho() per element; vectorize later if needed.
            for b in range(B):
                for h in range(H):
                    for w in range(W):
                        g = self.space.fibergroup.element(float(theta_n[b, h, w]))
                        # rho(g): (F x F) numpy, kernel: (F x 1) torch; convert rho to torch
                        rho_g = torch.tensor(self.rho(g), dtype=self.kernel_t.dtype, device=device)
                        row = (rho_g @ self.kernel_t).squeeze(-1)  # (F,)
                        A[b, h, w, n] = row
        return A

    # ---------- main forward ----------

    def forward(self, input: GeometricTensor) -> GeometricTensor:
        """
        input: GeometricTensor with type == self.in_type
        output: GeometricTensor with type == self.out_type
        """
        assert input.type == self.in_type, "input type mismatch"

        B, Cin, H, W = input.tensor.shape
        Fsize = self.rho.size

        # coefficient field view (B, C, F, H, W)
        x_hat = input.tensor.view(B, Cin, Fsize, H, W)

        # 1) predict base direction (type-1 field) and convert to base angle per location
        v = self.sampling_branch(input).tensor  # (B, 2, H, W)
        theta0 = self._angles_from_vector(v)    # (B, H, W)

        # 2) add fixed offsets to get N sample angles per location
        thetas = theta0[..., None] + self.offsets.view(1, 1, 1, self.N)  # (B, H, W, N)

        # 3) build A(x) : (B, H, W, N, F)
        A = self._build_A_rows_batch(thetas)

        # 4) samples s = A @ f̂  -> (B, C, N, H, W)
        # permute A to (B, N, H, W, F) then einsum with (B, C, F, H, W)
        s = torch.einsum('bnhwf,bcfhw->bcnhw', A.permute(0, 3, 1, 2, 4), x_hat)

        # 5) pointwise nonlinearity
        y = self._function(s)

        # 6) backprojection with fast pseudo-inverse ≈ (1/N) A^T
        # A^T: (B, F, H, W, N)
        AT = A.permute(0, 4, 1, 2, 3)
        y_hat = torch.einsum('bfhwn,bcnhw->bcfhw', AT, y) / float(self.N)

        out = y_hat.view(B, self.out_type.size, H, W)
        return GeometricTensor(out, self.out_type, input.coords)

    # ---- utilities ----

    def evaluate_output_shape(self, input_shape: Tuple[int, ...]) -> Tuple[int, ...]:
        assert len(input_shape) == 4 and input_shape[1] == self.in_type.size
        b, _, h, w = input_shape
        return (b, self.out_type.size, h, w)

    def check_equivariance(self, atol: float = 1e-5, rtol: float = 2e-2, assert_raise: bool = True) -> List[Tuple[Any, float]]:
    
        c = self.in_type.size
        B = 128
        x = torch.randn(B, c, *[3]*self.space.dimensionality)

        # since we mostly use non-linearities like relu or eu,l we make sure the average value of the features is
        # positive, such that, when we test inputs with only frequency 0 (or only low frequencies), the output is not
        # zero everywhere
        x = x.view(B, len(self.in_type), self.rho.size, *[3]*self.space.dimensionality)
        p = 0
        for irr in self.rho.irreps:
            irr = self.space.irrep(*irr)
            if irr.is_trivial():
                x[:, :, p] = x[:, :, p].abs()
            p+=irr.size

        x = x.view(B, self.in_type.size, *[3]*self.space.dimensionality)

        errors = []

        # for el in self.space.testing_elements:
        for _ in range(100):
            
            el = self.space.fibergroup.sample()
    
            x1 = GeometricTensor(x.clone(), self.in_type)
            x2 = GeometricTensor(x.clone(), self.in_type).transform_fibers(el)

            out1 = self(x1).transform_fibers(el)
            out2 = self(x2)

            out1 = out1.tensor.view(B, len(self.out_type), self.rho_out.size, *out1.shape[2:]).detach().numpy()
            out2 = out2.tensor.view(B, len(self.out_type), self.rho_out.size, *out2.shape[2:]).detach().numpy()

            errs = np.linalg.norm(out1 - out2, axis=2).reshape(-1)
            errs[errs < atol] = 0.
            norm = np.sqrt(np.linalg.norm(out1, axis=2).reshape(-1) * np.linalg.norm(out2, axis=2).reshape(-1))
            
            relerr = errs / norm

            # print(el, errs.max(), errs.mean(), relerr.max(), relerr.min())

            if assert_raise:
                assert relerr.mean()+ relerr.std() < rtol, \
                    'The error found during equivariance check with element "{}" is too high: max = {}, mean = {}, std ={}' \
                        .format(el, relerr.max(), relerr.mean(), relerr.std())

            # errors.append((el, errs.mean()))
            errors.append(relerr)

        # return errors
        return np.concatenate(errors).reshape(-1)

