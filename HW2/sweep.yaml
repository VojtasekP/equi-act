program: train.py
method: bayes
metric:
  name: val_loss
  goal: minimize

early_terminate:
  type: hyperband
  min_iter: 3

parameters:
  # --- optimization ---
  lr:
    distribution: log_uniform_values
    min: 0.00005
    max: 0.02        # wider than before; AdamW rarely needs >1e-2, but let Bayes try
  weight_decay:
    distribution: log_uniform_values
    min: 0.000001
    max: 0.05        # let it test stronger decay; AdamW can benefit on MNIST-rot

  # --- data / training ---
  batch_size:
    values: [64, 128, 256, 512]   # 512 often works on MNIST; drop if you OOM
  epochs:
    value: 50
  patience:
    value: 5

  # --- model ---
  max_rot_order:
    values: [1, 2, 3, 4]          # 4 widens space; watch compute
  channels_per_block:
    values:
      - [4, 8, 32]                # tiny
      - [8, 8, 8]                 # narrow/deep-ish
      - [8, 16, 64]
      - [12, 24, 96]
      - [16, 32, 128]
      - [24, 48, 128]
      - [32, 64, 128]
      - [32, 64, 256]
      - [48, 96, 192]
      - [64, 128, 256]            # beefy; keep an eye on memory
  layers_per_block:
    values: [1, 2, 3, 4]          # adds depth options; 2–3 usually sweet spot
  gated:
    values: [true, false]
  non_linearity:
    values: ["n_relu", "n_softplus"]   # only matters when gated=false
  kernel_size:
    values: [3, 5]
  pool_stride:
    value: 2
  pool_sigma:
    values: [0.5, 0.66, 0.9, 1.2]      # slightly wider
  invariant_channels:
    values: [32, 64, 128, 256]         # let head width move; 128–256 often best
  use_bn:
    value: true

  # fixed for MNIST
  n_classes:
    value: 10
